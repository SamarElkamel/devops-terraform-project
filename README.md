# DevOps Technical Test â€“ Containerized Data App (ETL + Web Visualization)

This project is a **personal adaptation of a previous data processing project** that I decided to **containerize and automate** as part of a **DevOps technical challenge**.

The goal is to demonstrate:
- The ability to **containerize a complete application** (multi-stage Dockerfile).
- The use of **Docker Compose** for orchestration.
- A **CI pipeline** (GitHub Actions) for build and lint automation.
- **Terraform** for simple cloud resource provisioning and **basic monitoring** setup.

---

## ğŸ“˜ Project Overview

Originally, this project performed a simple **ETL pipeline** using open data from the City of Paris and generated a small **static visualization** (front-end).  
For this challenge, I **enhanced and containerized** it to fit a modern DevOps workflow.

**Main stack:**
- **Python** â€“ for ETL scripts (data extraction, transformation, load)
- **MySQL 8.0** â€“ database for structured data storage
- **Chart.js (HTML/JS)** â€“ front-end visualization
- **Docker + Docker Compose** â€“ orchestration
- **GitHub Actions** â€“ CI automation (linting, testing, Docker build)
- **Terraform** â€“ deploy a simple S3 bucket for hosting the visualization

---

## ğŸ§± Architecture

.
â”œâ”€â”€ docker/
â”‚ â”œâ”€â”€ docker-compose.yml
â”‚ â”œâ”€â”€ init.sql
â”‚ â””â”€â”€ requirements.txt
â”œâ”€â”€ etl/
â”‚ â”œâ”€â”€ load_data_mysql.py
â”‚ â””â”€â”€ prepare_for_viz.py
â”œâ”€â”€ dataviz/
â”‚ â”œâ”€â”€ index.html
â”‚ â””â”€â”€ data_for_viz.json
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ run_etl.sh
â”œâ”€â”€ tests/
â”‚ â””â”€â”€ test_smoke.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .github/workflows/ci.yml
â”œâ”€â”€ terraform/
â”‚ â””â”€â”€ main.tf
â””â”€â”€ README.md


## ğŸ³ Containerization

### Multi-Stage Dockerfile
- Stage 1: build Python dependencies and wheel packages.
- Stage 2: create a minimal runtime image with only the required tools.
- Entry point: `scripts/run_etl.sh` (runs ETL + prepares data for visualization).

### Docker Compose
Defines 3 services:
- `db` â†’ MySQL database
- `etl` â†’ runs ETL job after database healthcheck
- `app` â†’ serves static visualization via a lightweight Python server

Run everything with:
```bash
docker compose -f docker/docker-compose.yml up --build
```

Then open `http://127.0.0.1:8000` to view the visualization.

Stop the stack:
```bash
docker compose -f docker/docker-compose.yml down
```

Persistent data: the `db_data` volume keeps MySQL data between runs.

---

## ğŸš€ Quickstart

1) Build and start services:
```bash
docker compose -f docker/docker-compose.yml up --build
```

2) Wait for the `etl` container to finish (it loads data and generates `dataviz/data_for_viz.json`).

3) Open the app: `http://127.0.0.1:8000`.

---

## âš™ï¸ Configuration

Environment variables (used by ETL and app):
- `MYSQL_HOST` (default `db`)
- `MYSQL_DATABASE` (default `tourismdb`)
- `MYSQL_USER` (default `root`)
- `MYSQL_PASSWORD` (default empty for local dev)
- `PORT` (static server, default `8000`)

You can override these in `docker/docker-compose.yml` or via `--env` flags.

---

## âœ… CI Pipeline (GitHub Actions)

Workflow at `.github/workflows/ci.yml` runs on push/PR:
- Lint with `ruff`
- Format check with `black --check`
- Tests with `pytest`
- Docker build (root Dockerfile) and `docker compose build`

Run the same locally:
```bash
pip install ruff black pytest
ruff check .
black --check .
pytest -q
docker build -t tourism-accessibility:local .
docker compose -f docker/docker-compose.yml build
```

---

## ğŸ§ª Tests

Minimal smoke test is provided:
```bash
docker compose -f docker/docker-compose.yml run --rm etl pytest -q
```

---

## ğŸ” Monitoring & Health

- Compose healthchecks:
  - `db`: `mysqladmin ping` ensures MySQL is up before ETL runs
  - `app`: HTTP healthcheck on `http://localhost:8000`
- Logs:
  - `docker logs tourism-app` / `tourism-etl` / `tourism-db`
  - In CI, inspect job logs for lint/test/build results
- Simple uptime idea:
  - Periodically hit `http://127.0.0.1:8000` (external monitor or cron) when deployed

---

## â˜ï¸ (Bonus) Terraform â€“ Static Hosting on S3

An example IaC is provided in `terraform/` to create a public S3 bucket configured for static website hosting (to host `dataviz/`).

Prerequisites: AWS credentials configured (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`).

Commands:
```bash
cd terraform
terraform init
terraform plan -var "bucket_name=<globally-unique-bucket-name>"
# terraform apply # to create resources
```

Upload artifacts (example):
```bash
aws s3 cp ../dataviz/index.html s3://<bucket>/index.html --acl public-read
aws s3 cp ../dataviz/data_for_viz.json s3://<bucket>/data_for_viz.json --acl public-read
```

Outputs:
- `website_endpoint` â€“ the S3 website URL

---

## ğŸ§° Troubleshooting

- Port 8000 in use: change `ports` mapping in `docker/docker-compose.yml`.
- ETL fails to connect to DB: ensure `db` is healthy; `docker ps` and `docker logs tourism-db`.
- CI fails on lint/format: run `ruff` and `black --check` locally and fix issues.

